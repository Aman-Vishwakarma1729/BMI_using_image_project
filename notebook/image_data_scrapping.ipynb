{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def scrape_images_from_url(url):\n",
    "    # Send a request to get the HTML content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the page.\")\n",
    "        return\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all image tags\n",
    "    img_tags = soup.find_all(\"img\")\n",
    "\n",
    "    # Create a directory to save the images\n",
    "    os.makedirs(\"scraped_images\", exist_ok=True)\n",
    "\n",
    "    # Download and save each image\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get(\"src\")\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(\"scraped_images\", img_name)\n",
    "\n",
    "            try:\n",
    "                urlretrieve(img_url, img_path)\n",
    "                print(f\"Downloaded: {img_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download: {img_url}\")\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrape =\n",
    "scrape_images_from_url(url_to_scrape)\n",
    " \"http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def image_scrapping(url):\n",
    "\n",
    "    for i in range(1,50):\n",
    "        if i==1:\n",
    "            response = requests.get(url)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(\"There is error is getting the URL\")\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "            img_tags = soup.find_all(\"img\")\n",
    "            \n",
    "\n",
    "            os.makedirs(\"scraped_images\", exist_ok=True)\n",
    "            for img_tag in img_tags:\n",
    "                img_url = img_tag.get(\"src\")\n",
    "                if img_url:\n",
    "                    img_url = urljoin(url, img_url)\n",
    "                    img_name = os.path.basename(img_url)\n",
    "                    img_path = os.path.join(\"scraped_images\", img_name)\n",
    "\n",
    "                    try:\n",
    "                        urlretrieve(img_url, img_path)\n",
    "                        print(f\"Downloaded: {img_url}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download: {img_url}\")\n",
    "                        print(f\"Error: {e}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Scrapping from page {i} is started\")\n",
    "            nxt_page = f\"/?page{i}\"\n",
    "            nex_pg_url = urljoin(url, nxt_page)\n",
    "            response = requests.get(nex_pg_url)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(\"There is error is getting the URL\")\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "            img_tags = soup.find_all(\"img\")\n",
    "            \n",
    "\n",
    "            os.makedirs(\"scraped_images\", exist_ok=True)\n",
    "            for img_tag in img_tags:\n",
    "                img_url = img_tag.get(\"src\")\n",
    "                if img_url:\n",
    "                    img_url = urljoin(url, img_url)\n",
    "                    img_name = os.path.basename(img_url)\n",
    "                    img_path = os.path.join(\"scraped_images\", img_name)\n",
    "\n",
    "                    try:\n",
    "                        urlretrieve(img_url, img_path)\n",
    "                        print(f\"Downloaded: {img_url}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download: {img_url}\")\n",
    "                        print(f\"Error: {e}\")\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping from page 2 is started\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/stat/1690395061\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/71822171.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/90172390.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/07196001.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/76810670.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/19341937.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/78379232.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/10499521.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/21787852.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/43201444.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/57816569.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/57295656.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/74527338.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/13556040.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/20098045.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/77764850.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/75972107.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/02185027.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/76325433.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/69911836.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/35210789.jpg\n",
      "Scrapping from page 3 is started\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/stat/1690395080\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/37027216.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/07007937.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/98562764.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/57487646.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/64540979.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/24256605.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/87678381.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/08162099.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/04181645.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/62924222.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/64498195.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/52372156.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/30631799.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/47660226.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/62011639.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/08834334.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/95842003.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/48193161.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/46836795.jpg\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/_pu/9/87677598.jpg\n",
      "Scrapping from page 4 is started\n",
      "Downloaded: http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/stat/1690395097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_scrapping(\u001b[39m\"\u001b[39;49m\u001b[39mhttp://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[141], line 60\u001b[0m, in \u001b[0;36mimage_scrapping\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     57\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mscraped_images\u001b[39m\u001b[39m\"\u001b[39m, img_name)\n\u001b[0;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     urlretrieve(img_url, img_path)\n\u001b[0;32m     61\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloaded: \u001b[39m\u001b[39m{\u001b[39;00mimg_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\BMI_from_image_project\\venv\\lib\\urllib\\request.py:276\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    273\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    275\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    278\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\BMI_from_image_project\\venv\\lib\\http\\client.py:454\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 454\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\BMI_from_image_project\\venv\\lib\\http\\client.py:498\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    493\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    495\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    500\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\BMI_from_image_project\\venv\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_scrapping(\"http://xn-----6kcczalffeh6afgdgdi2apgjghic4org.xn--p1ai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
